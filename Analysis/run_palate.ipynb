{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OmiDos import LoadSingleSample, seed_everything, OmiDosModel, ConvertATACTorchDataset, ConvertRNATorchDataset, PrepscATAC, RunDenoising, PrepscRNA, ConvertRNATorchBatchDataset, ConvertATACTorchBatchDataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from sklearn.cluster import KMeans\n",
    "import datetime\n",
    "import time\n",
    "import tracemalloc\n",
    "import psutil\n",
    "tracemalloc.start()\n",
    "process = psutil.Process(os.getpid())\n",
    "# folder_path = \"./data/scRNA/\"\n",
    "output_path = \"./output/\"\n",
    "# feature = \"human\"\n",
    "original_dir = \"/home/fanyi/1_paper/multi_omics/code/Palate_analysis/scScalpChromatin-main/results\"\n",
    "# 获取该文件夹下所有文件\n",
    "batch_size = 256\n",
    "imp = False\n",
    "Denoising = False\n",
    "pretrain = False\n",
    "lr:float = 0.0001\n",
    "seed_everything(1234)\n",
    "device = \"cuda:2\"\n",
    "file1 = \"Palate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file1)\n",
    "\n",
    "if not os.path.exists(os.path.join(output_path, file1)):\n",
    "    os.makedirs(os.path.join(output_path, file1))\n",
    "if not os.path.exists(os.path.join(output_path, file1)+'/model/'):\n",
    "    os.makedirs(os.path.join(output_path, file1)+'/model/')\n",
    "adata_x = LoadSingleSample(original_dir + '/scRNA_data/RNA.h5ad', trans=False)\n",
    "adata_y = LoadSingleSample(original_dir + '/scATAC_preprocessing/baseline_preprocessing/unfiltered_output/Tile.h5ad', trans=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_y.obs.index = adata_y.obs.index.str.split('#').str[1]\n",
    "# adata_x.obs.index = adata_x.obs.index.str.split('_').str[2]\n",
    "\n",
    "print(adata_x.shape)\n",
    "print(adata_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_indices = adata_y.obs.index.intersection(adata_x.obs.index)\n",
    "\n",
    "# Subsetting the DataFrames to keep only the common indices\n",
    "adata_x_common = adata_x.obs.loc[common_indices]\n",
    "adata_y_common = adata_y.obs.loc[common_indices]\n",
    "common_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_x = adata_x[adata_x.obs_names.isin(common_indices)].copy()\n",
    "adata_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_y = adata_y[adata_y.obs_names.isin(common_indices)].copy()\n",
    "adata_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "start_time = time.time()  \n",
    "########################\n",
    "\n",
    "if Denoising:\n",
    "    print(\"Run denoising part on the scRNA dataset.\")\n",
    "    adata = adata_x.copy()\n",
    "    inx, top_nvars, imputation_data = RunDenoising(adata, TF=False, DenosingModule_Dir=os.path.join(output_path, file1)+'/model/', \n",
    "                                                    original_dim=adata.shape[0], im_dim=8, num_epochs = 100, \n",
    "                                                    filter_num =5000, batch_size = 256, device = device, \n",
    "                                                    imputation=imp, sample_time=1)\n",
    "    # save inx\n",
    "    np.save(os.path.join(output_path, file1, file1 + '_inx.npy'), inx)\n",
    "    # save top_nvars\n",
    "    np.save(os.path.join(output_path, file1, file1 + '_top_nvars.npy'), top_nvars)\n",
    "    # save imputation_data\n",
    "    np.save(os.path.join(output_path, file1, file1 + '_imputation_data.npy'), imputation_data)\n",
    "    if inx is not None:\n",
    "        db_0 = adata[:,inx].copy()\n",
    "    else:\n",
    "        db_0 = adata\n",
    "    gene_selection = np.zeros(db_0.shape[1], dtype=bool)\n",
    "    gene_selection[top_nvars] = True\n",
    "    db_0.var['selected_genes'] = gene_selection\n",
    "    db_1 = db_0[:, db_0.var['selected_genes']]\n",
    "    print('Denoising dataset shape: {}'.format(db_1.shape))\n",
    "    print(\"Finished denoising part.\")\n",
    "    # using imputation data\n",
    "    if imp:\n",
    "        # adata = PrepscRNA(adata, size_factors=True, logtrans_input=False)\n",
    "        adata = imputation_data[:, db_0.var['selected_genes']]\n",
    "        min_cells = int(adata.shape[0] * 0.03) # 3% of the total number of cells\n",
    "        min_genes = int(adata.shape[1] * 0.03) # 3% of the total number of genes\n",
    "        sc.pp.filter_genes(db_1, min_counts=min_cells) # 1\n",
    "        sc.pp.filter_cells(db_1, min_counts=min_genes)\n",
    "        # Check which indices to keep in adata\n",
    "        indices_to_keep = adata.obs_names.isin(db_1.obs_names)\n",
    "        # Filter adata to only include these indices\n",
    "        adata = adata[indices_to_keep]\n",
    "        adata.layers[\"raw\"] = db_1.X.copy()\n",
    "        adata.obs['size_factors'] = db_1.obs.n_counts / np.median(db_1.obs.n_counts)\n",
    "        sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "        adata_x = adata\n",
    "    else:\n",
    "        print(\"Don't imputation.\")\n",
    "        min_cells = int(db_1.shape[0] * 0.03) # 3% of the total number of cells\n",
    "        min_genes = int(db_1.shape[1] * 0.03) # 3% of the total number of genes\n",
    "        adata_x = PrepscRNA(db_1, min_cells=min_cells, min_genes=min_genes, size_factors=True, logtrans_input=True)\n",
    "else:\n",
    "    print(\"Don't denoising.\")\n",
    "    min_cells = int(adata_x.shape[0] * 0.03) # 3% of the total number of cells\n",
    "    min_genes = int(adata_x.shape[1] * 0.03) # 3% of the total number of genes\n",
    "    adata_x = PrepscRNA(adata_x, min_cells=min_cells, min_genes=0, size_factors=True, logtrans_input=True)\n",
    "adata_y = PrepscATAC(adata_y, min_genes=2000, min_cells=30) # min_cell filter gene, min_genes filter cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save adata\n",
    "# adata_x.write(os.path.join(output_path, file1, file1 + '_preprocessed_x.h5ad'))\n",
    "# adata_y.write(os.path.join(output_path, file1, file1 + '_preprocessed_y.h5ad'))\n",
    "if 'batch' not in adata_x.obs:\n",
    "    adata_x.obs['batch'] = 'RNA'\n",
    "adata_x.obs['batch'] = adata_x.obs['batch'].astype('category')\n",
    "adata_x.obsm['onehot'] = np.stack(pd.Series([np.array([0, 1]) for _ in range(len(adata_x.obs.index))], index=adata_x.obs.index).values)\n",
    "if 'batch' not in adata_y.obs:\n",
    "    adata_y.obs['batch'] = 'ATAC'\n",
    "adata_y.obs['batch'] = adata_y.obs['batch'].astype('category')\n",
    "adata_y.obsm['onehot'] = np.stack(pd.Series([np.array([1, 0]) for _ in range(len(adata_y.obs.index))], index=adata_y.obs.index).values)\n",
    "trainloader_x = ConvertRNATorchBatchDataset(adata_x, batch_size=batch_size)\n",
    "trainloader_y, testloader_y = ConvertATACTorchBatchDataset(adata_y, batch_size=batch_size)\n",
    "cell_type = \"scRNA_scATAC\"\n",
    "alfa1 = 0.5\n",
    "beda1 = 1.1\n",
    "delta1 = 1.1\n",
    "epsi1 = 1.4\n",
    "sita1 = 0.0\n",
    "\n",
    "alfa2 = 0.2\n",
    "beda2 = 0.8\n",
    "delta2 = 1.1\n",
    "epsi2 = 1.0\n",
    "sita2 = 0.7\n",
    "latent = 32\n",
    "encode_dim = [256, 64] #1024, 128\n",
    "decode_x_dim = [64, 256] #128, 1024\n",
    "decode_y_dim = []\n",
    "# input_dim = adata.shape[1]\n",
    "dims = [adata_x.shape[1], adata_y.shape[1], latent, encode_dim, decode_x_dim, decode_y_dim]\n",
    "model = ModelmultiDEPF(dims=dims, n_centroids=10, device=device, \n",
    "                    binary=True, cell_type=cell_type)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_gmm_params(testloader_y,  device=device)\n",
    "model.fit(trainloader_x, trainloader_y, mode='source',\n",
    "        lr=lr, \n",
    "        max_iter= 200,\n",
    "        save_path=os.path.join(output_path, file1)+'/model/',\n",
    "        alfa = alfa1, beda=beda1, delta=delta1, epsi=epsi1, sita=sita1\n",
    "        )\n",
    "model.fit(trainloader_x, trainloader_y, mode='target',\n",
    "        lr=lr, \n",
    "        max_iter= 200,\n",
    "        save_path=os.path.join(output_path, file1)+'/model/',\n",
    "        alfa = alfa2, beda=beda2, delta=delta2, epsi=epsi2, sita=sita2\n",
    "        )\n",
    "#######################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "print(f\"Peak memory usage: {peak / 10**6} MB\")\n",
    "\n",
    "memory_info = process.memory_info()\n",
    "rss_memory = memory_info.rss  # Resident Set Size\n",
    "vms_memory = memory_info.vms  # Virtual Memory Size\n",
    "print(f\"RSS memory usage: {rss_memory / 10**6} MB\")\n",
    "print(f\"VMS memory usage: {vms_memory / 10**6} MB\")\n",
    "#######################################\n",
    "\n",
    "adata_x.obsm['latent_share'] = model.EmbedingSharescRNA(adata_x.X.toarray(), batch_size=batch_size).cpu().numpy()\n",
    "adata_x.obsm['latent_Single'] = model.EmbedingSinglescRNA(adata_x.X.toarray(), batch_size=batch_size).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_x.__dict__['_raw'].__dict__['_var'] = adata_x.__dict__['_raw'].__dict__['_var'].rename(columns={'_index': 'features'})\n",
    "adata_x.write(os.path.join(output_path, file1, file1 + '_Embeding_x.h5ad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_y.obsm['latent_share'] = model.EmbedingSharescATAC(testloader_y, device=device, out='z')\n",
    "adata_y.obsm['latent_single'] = model.EmbedingSinglescATAC(testloader_y, device=device, out='z')\n",
    "adata_y.write(os.path.join(output_path, file1, file1 + '_Embeding_y.h5ad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_x = sc.read_h5ad(os.path.join(output_path, file1, file1 + '_Embeding_x.h5ad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_y = sc.read_h5ad(os.path.join(output_path, file1, file1 + '_Embeding_y.h5ad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata_x\n",
    "sc.pp.neighbors(adata, n_neighbors=30, use_rep='latent_share')\n",
    "sc.tl.leiden(adata)\n",
    "sc.tl.umap(adata, min_dist=0.2)\n",
    "color = [c for c in ['leiden'] if c in adata.obs]\n",
    "sc.pl.umap(adata, color=color, show=True, wspace=0.4, ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata, n_neighbors=30, use_rep='latent_Single')\n",
    "sc.tl.leiden(adata)\n",
    "sc.tl.umap(adata, min_dist=0.2)\n",
    "color = [c for c in ['leiden'] if c in adata.obs]\n",
    "sc.pl.umap(adata, color=color, show=True, wspace=0.4, ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata_y\n",
    "sc.pp.neighbors(adata, n_neighbors=30, use_rep='latent_share')\n",
    "sc.tl.leiden(adata)\n",
    "sc.tl.umap(adata, min_dist=0.2)\n",
    "color = [c for c in ['leiden'] if c in adata.obs]\n",
    "sc.pl.umap(adata, color=color, show=True, wspace=0.4, ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata, n_neighbors=30, use_rep='latent_single')\n",
    "sc.tl.leiden(adata)\n",
    "sc.tl.umap(adata, min_dist=0.2)\n",
    "color = [c for c in ['leiden'] if c in adata.obs]\n",
    "sc.pl.umap(adata, color=color, show=True, wspace=0.4, ncols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata_x\n",
    "sc.pp.neighbors(adata, n_neighbors=30, use_rep='latent_Single')\n",
    "sc.tl.leiden(adata)\n",
    "sc.tl.umap(adata, min_dist=0.2)\n",
    "color = [c for c in ['leiden'] if c in adata.obs]\n",
    "sc.pl.umap(adata, color=color, save=file1+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")+\"_Single_scRNA.pdf\", show=True, wspace=0.4, ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata_x\n",
    "sc.pp.neighbors(adata, n_neighbors=30, use_rep='latent_share')\n",
    "sc.tl.leiden(adata, resolution=0.5)\n",
    "sc.tl.umap(adata, min_dist=0.2)\n",
    "color = [c for c in ['cell_type_rna', 'kmeans', 'leiden'] if c in adata.obs]\n",
    "sc.pl.umap(adata, color=color, save=file1+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")+\"__Share_scRNA.pdf\", show=True, wspace=0.4, ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(adata.obs.index.values).to_csv(os.path.join(output_path, file1, file1 + '_selectCellid.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save adata.obs['leiden'] to csv\n",
    "adata.obs['leiden'].to_csv(os.path.join(output_path, file1, file1 + '_Share_scRNA_leiden_clusters.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata_y\n",
    "sc.pp.neighbors(adata, n_neighbors=30, use_rep='latent_single')\n",
    "sc.tl.leiden(adata, resolution=1.1)\n",
    "sc.tl.umap(adata, min_dist=0.2)\n",
    "color = [c for c in ['leiden'] if c in adata.obs]\n",
    "sc.pl.umap(adata, color=color, save=file1+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")+\"_Single_scATAC.pdf\", show=True, wspace=0.4, ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata_y\n",
    "sc.pp.neighbors(adata, n_neighbors=30, use_rep='latent_share')\n",
    "sc.tl.leiden(adata, resolution=1.1)\n",
    "sc.tl.umap(adata, min_dist=0.2)\n",
    "color = [c for c in ['leiden'] if c in adata.obs]\n",
    "sc.pl.umap(adata, color=color, save=file1+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")+\"_Share_scATAC.pdf\", show=True, wspace=0.4, ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save adata.obs['leiden'] to csv\n",
    "adata.obs['leiden'].to_csv(os.path.join(output_path, file1, file1 + '_Share_scATAC_leiden_clusters.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设 adata_y 是你的AnnData对象\n",
    "# 提取 latent_share 数据\n",
    "latent_share_data = adata_y.obsm['latent_share']\n",
    "\n",
    "# 为列名生成一个列表，假设有多个特征\n",
    "column_names = [f\"latent{i+1}\" for i in range(latent_share_data.shape[1])]\n",
    "\n",
    "# 将 numpy array 转换为 DataFrame\n",
    "latent_share_df = pd.DataFrame(latent_share_data, columns=column_names)\n",
    "\n",
    "# 设置DataFrame的行名为adata_y.obs中的某个列，假设这个列名是'cell_id'\n",
    "latent_share_df.index = adata_y.obs.index\n",
    "\n",
    "# 保存 DataFrame 到 CSV 文件\n",
    "latent_share_df.to_csv(os.path.join(output_path, file1, file1 + \"_latent_share_data_ATAC.csv\"))\n",
    "\n",
    "print(\"Data saved to 'latent_share_data.csv', with cell IDs and dynamically specified column names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设 adata_y 是你的AnnData对象\n",
    "# 提取 latent_share 数据\n",
    "latent_share_data = adata_y.obsm['latent_single']\n",
    "\n",
    "# 为列名生成一个列表，假设有多个特征\n",
    "column_names = [f\"latent{i+1}\" for i in range(latent_share_data.shape[1])]\n",
    "\n",
    "# 将 numpy array 转换为 DataFrame\n",
    "latent_share_df = pd.DataFrame(latent_share_data, columns=column_names)\n",
    "\n",
    "# 设置DataFrame的行名为adata_y.obs中的某个列，假设这个列名是'cell_id'\n",
    "latent_share_df.index = adata_y.obs.index\n",
    "\n",
    "# 保存 DataFrame 到 CSV 文件\n",
    "latent_share_df.to_csv(os.path.join(output_path, file1, file1 + \"_latent_private_data_ATAC.csv\"))\n",
    "\n",
    "print(\"Data saved to 'latent_private_data.csv', with cell IDs and dynamically specified column names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设 adata_y 是你的AnnData对象\n",
    "# 提取 latent_share 数据\n",
    "latent_share_data = adata_x.obsm['latent_share']\n",
    "\n",
    "# 为列名生成一个列表，假设有多个特征\n",
    "column_names = [f\"latent{i+1}\" for i in range(latent_share_data.shape[1])]\n",
    "\n",
    "# 将 numpy array 转换为 DataFrame\n",
    "latent_share_df = pd.DataFrame(latent_share_data, columns=column_names)\n",
    "\n",
    "# 设置DataFrame的行名为adata_y.obs中的某个列，假设这个列名是'cell_id'\n",
    "latent_share_df.index = adata_x.obs.index\n",
    "\n",
    "# 保存 DataFrame 到 CSV 文件\n",
    "latent_share_df.to_csv(os.path.join(output_path, file1, file1 + \"_latent_share_data_RNA.csv\"))\n",
    "\n",
    "print(\"Data saved to 'latent_share_data.csv', with cell IDs and dynamically specified column names.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyPackage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
